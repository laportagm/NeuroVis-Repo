Okay, you've made excellent progress through Phase 2, and your application can now display curated information about selected brain structures\!

Now, let's dive into the exciting **Phase 3: In-App AI Assistant (via Online API) & Basic Q\&A**. This phase will introduce a dynamic layer of intelligence to your application, allowing users to get AI-generated explanations and answers related to neuroanatomy.

---

### `PHASE_3_IN_APP_AI_ASSISTANT_ONLINE_API.md`

**Project:** AI-Enhanced Brain Anatomy Visualizer (Desktop)
**Phase:** 3 - In-App AI Assistant (via Online API) & Basic Q\&A
**Version:** 1.0
**Date:** Thursday, May 15, 2025
**Location:** Ardmore, Pennsylvania, United States

**Phase Goal:** To integrate an external, free-tier Large Language Model (LLM) API to act as an in-app AI assistant. This assistant will provide simple, dynamically generated explanations for selected brain structures and answer basic user questions about them. This involves researching and selecting an API, implementing API call logic, designing initial prompts, displaying AI responses in the UI, and basic error handling for online interactions. This phase addresses "Must-Have" feature M7.

---

**1. Introduction**

Phase 3 elevates your application from a static information viewer to an interactive learning tool with dynamic content generation. By connecting to an online AI, you can offer users explanations and answers that go beyond your curated local knowledge base. This phase introduces concepts like API communication, asynchronous operations, and basic prompt engineering. The focus is on leveraging a _free-tier_ online service to keep costs at zero for you and the end-user, which means being mindful of API usage limits and the need for an internet connection for these AI features.

---

**2. Prerequisites for Starting Phase 3**

- **[ ] All Phase 2 tasks and checkpoints are completed and "perfected."**
  - Application loads, displays, and allows interaction with the 3D model.
  - Selecting a 3D part highlights it and correctly displays its `displayName`, `shortDescription`, and `functions` from the local `anatomical_data.json` file in a designated UI panel.
  - The local knowledge base (`anatomical_data.json`) is populated for at least 5-10 structures.
  - Project is stable and under Git version control with Phase 2 work committed.
  - You are comfortable with your chosen framework (Electron + Three.js OR Godot Engine).

---

**3. Applying Core Development Principles in Phase 3**

- **Feature Categorization/Boundaries:** Focus _only_ on M7 (In-app AI assistant via online API for basic explanations/Q\&A). The AI's capabilities will be simple initially. No complex conversational AI or local AI models.
- **Performance:** API calls are asynchronous and must not freeze the UI. Response times depend on the API and internet; use loading indicators.
- **UI/UX Sketching:** Plan where and how AI responses will be displayed. How will users trigger the AI (e.g., a button after selection, a simple input field)? Refer to `UI_UX_DESIGN_NOTES.md` and update it.
- **Data Management:** Managing the API key securely (as much as possible). Handling JSON request/response payloads for the API.
- **Walking Skeleton:** Adds a significant new capability: dynamic, AI-generated content, making the skeleton more intelligent.
- **Incremental Development:** Start with making _any_ successful API call. Then integrate it with selections. Then refine UI display.
- **AI-Assisted Development:** Crucial for understanding API docs, generating HTTP request code, handling async operations, and drafting prompts.
- **Clean, Readable, Maintainable Code:** Encapsulate all API interaction logic into a dedicated module/class (`AIService`).
- **Error Handling:** Essential for network issues, API errors, rate limits, invalid responses. Provide clear user feedback.
- **Testing and Debugging:** Test API calls with tools like Postman (optional) or directly in code. Debug async logic carefully.
- **Deployment Strategy Considerations:** The need for an internet connection for AI features becomes a key point. How API keys are handled in a distributed app is a concern (see Task 2).
- **AI for Documentation:** Update `TECHNICAL_SPECIFICATIONS.md` (with chosen API details), `AI_MODULARITY_FOR_BUILDING.md` (with `AIService` design), and `README.md`.

---

**4. Key Concepts to Learn/Understand in Phase 3**

- **API (Application Programming Interface):** How different software components communicate. In this case, your app (client) communicates with an LLM service (server).

- **HTTP/HTTPS:** The protocol used for communication over the web. You'll be making HTTP requests (likely POST).

- **JSON Payloads:** Data sent to and received from APIs is commonly formatted as JSON.

- **API Keys:** Secret tokens used to authenticate your application with an API service and track usage.

- **Asynchronous Operations:** Network requests take time. Your application must not freeze while waiting for a response.

  - **Electron/JavaScript:** `async/await`, `Promises`, `Workspace` API.
  - **Godot/GDScript:** `HTTPRequest` node (uses signals for responses), or `await` with `Signal` in Godot 4.x.

- **RESTful APIs (Common Paradigm):** A common way of designing APIs using standard HTTP methods (GET, POST, PUT, DELETE). You'll likely use POST.

- **HTTP Headers:** Part of an HTTP request that carries metadata (e.g., `Content-Type: application/json`, `Authorization: Bearer YOUR_API_KEY`).

- **Status Codes:** HTTP response codes indicating success (2xx, e.g., 200 OK) or failure (4xx client errors, e.g., 401 Unauthorized, 429 Too Many Requests; 5xx server errors).

- **Prompt Engineering (Basic):** Crafting effective input text (prompts) to guide the LLM to produce the desired output.

- **Rate Limiting/Quotas:** Free tier APIs usually have limits on how many requests you can make per minute/day/month.

- **_AI Assist Prompt Idea for any concept:_** "Explain what an API key is and why it's important for using services like an LLM API. What are common ways to send an API key in an HTTP request?" OR "Explain asynchronous programming in [JavaScript using async/await / GDScript using HTTPRequest node and signals or await]. Why is it needed for API calls?"

---

**5. Essential Tools & Resources for Phase 3**

- **[ ] Your Chosen Code Editor (VS Code).**
- **[ ] Your Chosen Framework (Electron or Godot).**
- **[ ] Documentation for the LLM API you choose in Task 1.** This is CRITICAL.
- **[ ] A way to make test API calls (Optional but Recommended):**
  - **Postman / Insomnia:** GUI tools for sending HTTP requests and viewing responses. Helps test API calls outside your app first. (Both have free versions).
  - **curl:** Command-line tool for transferring data with URLs.
- **[ ] Internet Connection (for development and for the AI features in your app).**
- **[ ] Secure place to temporarily store your API key during development (e.g., an environment variable, a local untracked config file â€“ NOT directly in versioned code).**

---

**6. Detailed Tasks, Checkpoints & AI Integration**

**Task 1: Research, Select, and Sign Up for a Free-Tier LLM API**

- **Activity:** Identify LLM providers offering APIs with free tiers suitable for generating short explanations or answering simple questions. Consider factors like ease of use, quality of output for your needs, and clarity of free tier limitations. Sign up and obtain an API key.
- **Research:** Search for "free tier LLM API for text generation," "generative AI API free," etc.
  - **Potential candidates (check current free tiers and terms):** OpenAI (ChatGPT models), Google AI (Gemini models), Anthropic (Claude models), Cohere, Groq (hosts various open models), Fireworks AI, OpenRouter (provides access to many models).
- **AI Assistant Role:**
  - "Can you list some current LLM API providers that offer a free tier suitable for a hobbyist desktop application to generate short text explanations (2-3 sentences)? What are typical free tier limits I should be aware of (e.g., requests per month, tokens per request)?"
  - "I'm looking at [Specific LLM API Provider]'s documentation for their [Model Name] API. Can you help me understand how to structure a basic request to get a text completion?"
- **Checkpoint/Perfection Criteria:**
  - **[ ]** At least one LLM API provider has been chosen.
  - **[ ]** You have successfully signed up for their service.
  - **[ ]** You have obtained an API key and stored it securely _outside_ of your Git repository (e.g., in a local environment variable or an untracked config file for now).
  - **[ ]** You have reviewed the API documentation regarding request format, authentication, and free tier usage limits.
  - **[ ]** You have an idea of the API endpoint URL you'll need to call.

**Task 2: Design API Interaction Module & Basic API Key Handling**

- **Activity:** Plan and create a separate module/script in your project (e.g., `AIService.js` or `ai_service.gd`) that will encapsulate all logic for communicating with the LLM API. Outline a basic strategy for handling the API key within your desktop application context.
- **Data Management:** The API key is sensitive.
- **AI Modularity:** This task directly implements principles from `AI_MODULARITY_FOR_BUILDING.md` for the in-app AI feature.
- **Clean Code:** A dedicated module keeps API logic separate and organized.
- **Steps:**
  1.  **[ ] Create `AIService` File:** Create the empty script file in an appropriate location in your project.
  2.  **[ ] API Key Handling Strategy (Development vs. Distribution):**
      - **Development:** Load the API key from an environment variable or a local, untracked configuration file (e.g., `config.dev.json` listed in `.gitignore`).
      - **Distribution (Challenging for bundled client-side keys):**
        - **Option 1 (Less Secure, for personal/hobbyist free-tier projects where stakes are low):** Bundle the key, perhaps slightly obfuscated (not true security). _Be aware of the risk that a determined user could extract it._ This is often discouraged.
        - **Option 2 (Better, if API allows):** Proxy requests through your own minimal serverless function where the key is stored securely (adds complexity and potential tiny cost if free tier exceeded). _Likely too complex for MVP._
        - **Option 3 (User-Provided Key):** Design the app so users can enter their _own_ free-tier API key from the provider (requires UI for this).
        - **For MVP:** Focus on the development strategy (environment variable). Make a note in `TECHNICAL_SPECIFICATIONS.md` about the distribution challenge for API keys.
      - **_AI Assist:_** "For my [Electron/Godot] desktop app, I need to use an API key for an LLM.
        1.  For development, how can I load an API key from an environment variable named `MY_LLM_API_KEY` into my [JavaScript/GDScript] code?
        2.  What are the security challenges of bundling an API key directly in a distributed desktop application, even for a free tier? Are there any simple, low-effort ways to make it slightly harder to find for a very basic project, or is it better to plan for users to input their own key later?"
- **Checkpoint/Perfection Criteria:**
  - **[ ]** An empty `AIService.js` (or `ai_service.gd`) file is created and committed.
  - **[ ]** You have a working method to load your API key for development (e.g., from an environment variable) into this module.
  - **[ ]** You understand the basic security implications of API key handling in desktop apps and have noted your initial approach.

**Task 3: Implement Basic API Call Functionality**

- **Activity:** Write a core function within your `AIService` module that can send a test prompt to the selected LLM API and log the raw JSON response or any errors. This is your first "live fire" test of the API from your code.

- **Asynchronous Programming:** This is key. UI must remain responsive.

- **Error Handling:** Check for network errors, HTTP status code errors from the API.

- **Steps (Choose A or B):**

  **A. Electron + Three.js Path (in `AIService.js` or similar, callable from `renderer.js` via preload if key is in main):** \* If API key is managed in the main process (recommended for better security if bundled):
  1\. **[ ] `main.js` function to make API call:** Create a function in `main.js` that takes a prompt, uses Node.js `https` or a library like `axios`/`node-fetch` to make the POST request with the API key from an environment variable.
  2\. **[ ] IPC Setup:** Use `ipcMain.handle` in `main.js` to expose this function to the renderer. In `preload.js`, expose the IPC invoker to `renderer.js`.
  3\. **[ ] `renderer.js` calls via IPC:** `renderer.js` calls the exposed function. \* Simpler (but less secure for key if bundled directly in renderer accessible code): API call directly from `renderer.js` `AIService.js` using `Workspace`. \* **_AI Assist (Direct fetch from renderer for simplicity first):_** "In a JavaScript `AIService.js` module for my Electron renderer, show me an `async function callLLM(promptText)` that:
  1\. Retrieves an API key (assume it's available in a variable `MY_API_KEY` loaded from env var for now).
  2\. Makes a `Workspace` POST request to `[YOUR_CHOSEN_API_ENDPOINT_URL]`.
  3\. Includes headers: `Content-Type: application/json` and `Authorization: Bearer ${MY_API_KEY}`.
  4\. Sends a JSON body like `{ "prompt": promptText, "max_tokens": 50 }` (adjust payload based on your API's docs).
  5\. Logs the full JSON response or any error to the console.
  Explain how to handle the Promise from `Workspace` using `async/await` and `try/catch` for errors."

  **B. Godot Engine Path (in `ai_service.gd`):**
  1\. **[ ] Implement `HTTPRequest` Node Logic:**
  \`\`\`gdscript
  \# In ai_service.gd (likely an Autoload or a node in your main scene)
  extends Node

  ````
        var api_key = "" # Loaded from OS.get_environment("MY_LLM_API_KEY") in _ready()
        var api_url = "YOUR_CHOSEN_API_ENDPOINT_URL"
        var http_request: HTTPRequest

        func _ready():
            api_key = OS.get_environment("MY_LLM_API_KEY")
            if api_key == null or api_key.is_empty():
                printerr("API Key not found in environment variable MY_LLM_API_KEY")

            http_request = HTTPRequest.new()
            add_child(http_request)
            http_request.request_completed.connect(self._on_request_completed) # Godot 4.x
            # For Godot 3.x: http_request.connect("request_completed", self, "_on_request_completed")


        func call_llm(prompt_text: String):
            if api_key.is_empty():
                print("API Key missing, cannot call LLM.")
                # Optionally emit a signal or return an error state
                return

            var headers = [
                "Content-Type: application/json",
                "Authorization: Bearer " + api_key
            ]
            var body = JSON.stringify({ # Godot 4.x
            # For Godot 3.x: var body = JSON.print({
                "prompt": prompt_text,
                "max_tokens": 50, # Adjust per API docs
                # Other parameters like model, temperature, etc. as per API docs
            })

            # Clear previous request data if any (good practice, though new() does this)
            # http_request.cancel_request() # if a request might be pending

            var error = http_request.request(api_url, headers, HTTPClient.METHOD_POST, body)
            if error != OK:
                printerr("Error starting HTTPRequest: ", error)
                # Optionally emit a signal or return an error state
            else:
                print("LLM API request sent for prompt: ", prompt_text)
                # Add a loading indicator in UI here

        func _on_request_completed(result, response_code, headers, body_data: PackedByteArray):
            # Remove loading indicator from UI here
            if result == HTTPRequest.RESULT_SUCCESS and response_code >= 200 and response_code < 300:
                var response_body_text = body_data.get_string_from_utf8()
                var json = JSON.new()
                var parse_error = json.parse(response_body_text) # Godot 4.x
                # For Godot 3.x: parse_error = json.parse_string(response_body_text)

                if parse_error == OK:
                    var response_data = json.get_data() # Godot 4.x
                    # For Godot 3.x: response_data = json.result
                    print("LLM API Success Response: ", response_data)
                    # TODO: Extract text and send to UI (e.g., via a signal)
                    # Example: emit_signal("ai_response_ready", response_data.choices[0].text) if that's the structure
                else:
                    printerr("LLM API JSON Parse Error: ", json.get_error_message(), " at line ", json.get_error_line())
            else:
                printerr("LLM API Request Failed. Result: ", result, " Code: ", response_code)
                var error_body = body_data.get_string_from_utf8()
                printerr("Error Body: ", error_body)

        # Define a signal if you want to send data back to UI
        # signal ai_response_ready(response_text)
        ```
    * ***AI Assist:*** "Provide GDScript (Godot 4.x) for an Autoload node `AIService.gd`. It should:
        1.  Load an API key from `OS.get_environment("MY_LLM_API_KEY")` in `_ready()`.
        2.  Have a function `call_llm(prompt_text: String)`.
        3.  This function uses an `HTTPRequest` child node to make a POST request to `[YOUR_API_ENDPOINT]` with headers `Content-Type: application/json` and `Authorization: Bearer [API_KEY]`.
        4.  The JSON body should be `{"prompt": promptText, "max_tokens": 50}` (or similar based on chosen API).
        5.  Connect to the `request_completed` signal. The callback `_on_request_completed` should log the `response_code` and the parsed JSON body if successful (2xx code), or log errors.
        Explain the role of `HTTPRequest` and its signals."
  ````

- **Checkpoint/Perfection Criteria:**

  - **[ ]** You can trigger the `callLLM` function from your application (e.g., via a temporary button or a console command).
  - **[ ]** The API request is sent successfully (monitor network traffic if possible, or rely on logs).
  - **[ ]** The raw JSON response from the LLM API (or an error message) is successfully logged to your application's console/output.
  - **[ ]** Basic error handling for network/API failures is in place and logs appropriate messages.

**Task 4: Design Initial Prompts & Integrate with Selection**

- **Activity:** When a brain structure is selected by the user (from Phase 1/2 functionality), dynamically generate a simple prompt for the LLM and trigger the `callLLM` function.
- **Data Management:** Using the `displayName` of the selected structure (from your local knowledge base) to construct the prompt.
- **Walking Skeleton:** AI now responds to direct user interaction with the 3D model.
- **Steps:**
  1.  **[ ] Design a Simple Prompt Template:**
      - Example: `f"Explain the primary functions of the {structure_name} in one or two simple sentences, suitable for a student."` (Python f-string example, adapt to JS/GDScript string formatting).
  2.  **[ ] Modify Selection Logic:** In the code where a 3D structure is selected and its local info is displayed (from Phase 2):
      - Get the `displayName` of the selected structure.
      - Construct your prompt using this name.
      - Call your `AIService.callLLM(constructedPrompt)`.
      - Add a UI element (e.g., a button "Get AI Explanation") that triggers this, or do it automatically after local info is shown (consider API limits for automatic calls). For now, a button is safer.
      - **_AI Assist:_** "When a brain structure is selected, I get its `displayName` (e.g., 'Hippocampus') from my local data.
        1.  Help me write a good, concise prompt template to ask an LLM to 'Explain the main functions of the [structure\_name] in 1-2 simple sentences.'
        2.  In my [Electron `renderer.js` selection callback / Godot main scene script's selection handling], after displaying local info, show how to construct this prompt with the selected structure's name and then call my existing `AIService.callLLM(prompt)` function. For now, let's assume I'll add a button that the user clicks to trigger this AI call."
- **Checkpoint/Perfection Criteria:**
  - **[ ]** Selecting a brain structure and then triggering the AI call (e.g., via a new button) results in an API request being sent with a prompt relevant to that specific structure.
  - **[ ]** The LLM API response (or error) for this dynamic prompt is logged to the console.

**Task 5: Display AI Response in UI & Handle Loading/Error States**

- **Activity:** Parse the actual text content from the LLM API's JSON response. Update your UI panel (from Phase 2) or a new dedicated UI area to display this AI-generated explanation. Implement visual feedback for loading and error states.
- **UI/UX:** Clearly differentiate AI-generated content from your curated local data. Loading indicators are crucial. Error messages should be user-friendly.
- **Error Handling:** If API fails or returns unexpected data, show an appropriate message in the UI.
- **Clean Code:** Modify/extend your `updateInfoPanel` or create a new `displayAIResponse` function.
- **Steps:**
  1.  **[ ] Modify `AIService` for Structured Response:**
      - Your `AIService`'s API call completion handler (`_on_request_completed` in Godot, or the `then/catch` for `Workspace` in JS) needs to parse the JSON response and extract the actual generated text (this depends heavily on the specific API's response structure â€“ e.g., `response.choices[0].text` or `response.generations[0].text`).
      - Instead of just logging, have `AIService` return the extracted text (or an error object/message).
      - **Electron/JS:** The `async callLLM` function should `return extractedText;` or `throw new Error(errorMessage);`.
      - **Godot/GDScript:** The `_on_request_completed` function should `emit_signal("ai_response_ready", extracted_text)` or `emit_signal("ai_request_failed", error_message)`. Your main script will connect to these signals.
  2.  **[ ] Add UI Elements for AI Response & Status:**
      - In your info panel (or a new UI area), add an element to display the AI's text (e.g., a `<p id="ai-response-text"></p>` or a Godot `RichTextLabel` named `AIResponseTextLabel`).
      - Add a temporary loading indicator element (e.g., `<p id="ai-loading-indicator" style="display:none;">Loading AI explanation...</p>` or a Godot `Label` `AILoadingIndicator`).
  3.  **[ ] Update UI with AI Response / Errors:**
      - When the AI call is initiated: Show the loading indicator, hide previous AI response/error.
      - When `AIService` provides the AI text: Hide loading indicator, display the AI text.
      - If `AIService` reports an error: Hide loading indicator, display a user-friendly error message (e.g., "AI assistant is currently unavailable. Please check your internet connection.").
      - **_AI Assist:_** "My `AIService.callLLM()` now [returns a Promise resolving to the AI's text / emits a signal `ai_response_ready(text)` or `ai_request_failed(error)`]. In my main UI script:
        1.  Show me how to display a 'Loading AI...' message in a `<p id='ai-loading-indicator'>` when the call starts.
        2.  When the AI text is received, hide the loading message and display the text in `<p id='ai-response-text'>`.
        3.  If an error occurs, display an error message in `<p id='ai-response-text'>`.
            (Provide similar logic for Godot using signals and Label/RichTextLabel nodes)."
- **Checkpoint/Perfection Criteria:**
  - **[ ]** When the AI feature is triggered for a selected structure:
    - A "loading" indicator appears in the UI.
    - After a short delay, the AI-generated explanation for that structure is displayed clearly in the designated UI area.
  - **[ ]** If the API call fails (e.g., no internet, API key issue, API server error), a user-friendly error message is shown in the UI instead of the app crashing or freezing.
  - **[ ]** The UI correctly handles transitions between loading, success, and error states for the AI feature.

---

**7. Common Challenges & Troubleshooting in Phase 3:**

- **API Key Issues:** Invalid key, incorrect inclusion in headers, exceeding free tier quota for the key. (API usually returns 401 Unauthorized or 403 Forbidden).
- **Network Errors:** No internet connection, DNS issues, firewall blocking requests. (Code will usually throw a network error).
- **Incorrect API Endpoint/Request Format:** Wrong URL, incorrect HTTP method (e.g., GET instead of POST), badly formatted JSON payload, missing required headers. (API returns 400 Bad Request or 404 Not Found).
- **Asynchronous Code Problems:** UI freezing (if API call is blocking), race conditions, callbacks/promises/signals not handled correctly.
- **Parsing API Response:** The structure of the JSON response from LLMs varies. You need to inspect it carefully (e.g., `console.log(rawData)`) to find the path to the actual generated text.
- **Rate Limiting:** Making too many API calls in a short period can lead to 429 Too Many Requests errors. Implement delays or user-initiated calls if this happens.
- **_AI Assist for Debugging:_** "I'm trying to call an LLM API. My [Electron app / Godot app] gets a [HTTP status code, e.g., 401] or [network error message]. Here's my request code: [paste code for `AIService.callLLM`]. My API key variable is set. What could be wrong? How can I log the exact request headers and body being sent for debugging?"

**8. Leveraging Your AI Assistants Effectively in Phase 3:**

- **Understanding API Documentation:** "This API documentation says I need to send a 'bearer token'. How do I format that in an HTTP Authorization header in [JavaScript `Workspace` / Godot `HTTPRequest`]?"
- **Writing Asynchronous Code:** "Explain how `async/await` in JavaScript helps manage my API call to the LLM. What happens if I don't use `await`?" or "How do I properly connect to and handle the `request_completed` signal from Godot's `HTTPRequest` node, especially for success and error cases?"
- **Drafting Prompts:** "Suggest 3 different simple prompts I could use to ask an LLM to explain a brain structure like the 'amygdala' for a beginner student. Focus on clarity and conciseness."
- **Parsing JSON Responses:** "The LLM API returns this JSON: `[paste example JSON response]`. How do I access the main generated text which seems to be at `result.outputs[0].text` in [JavaScript/GDScript]?"
- **Error Handling Strategies:** "What are common HTTP errors I should specifically handle when calling an external API, and how can I show user-friendly messages for them?"

**9. Phase 3 Completion Criteria / "Definition of Done"**

You have "perfected" and completed Phase 3 when:

- **[ ]** You have successfully selected, signed up for, and obtained an API key for a free-tier LLM API service.
- **[ ]** Your application has a dedicated module/script (`AIService`) that can successfully make authenticated calls to this LLM API.
- **[ ]** API key handling for development is functional (e.g., via environment variables).
- **[ ]** When a user selects a documented brain structure and triggers the AI feature (e.g., via a button):
  - A relevant prompt, incorporating the selected structure's name, is sent to the LLM API.
  - A UI loading indicator is displayed while waiting for the API response.
  - The AI-generated textual explanation is correctly parsed and displayed in a designated UI area.
- **[ ]** The application gracefully handles common API/network errors by displaying user-friendly messages in the UI rather than crashing or freezing.
- **[ ]** The core functionality from Phase 1 and 2 (3D interaction, display of local KB info) remains intact and functional.
- **[ ]** All code developed in this phase is committed to Git with clear messages.
- **[ ]** Key documentation (`TECHNICAL_SPECIFICATIONS.md`, `AI_MODULARITY_FOR_BUILDING.md`, `UI_UX_DESIGN_NOTES.md`, `README.md`) is updated to reflect the new AI feature and API integration.

---

**10. Next Steps (Transition to Phase 4)**

Fantastic\! Your application now has a dynamic AI component, making it significantly more interactive and capable. This is a huge achievement, especially handling external APIs and asynchronous operations.

The next stage is **Phase 4: Refinement, Testing, Packaging, & Pre-Release Documentation**. In Phase 4, you will focus on:

- Thoroughly testing all MVP features (from Phases 1, 2, and 3).
- Refining the UI/UX for better usability and a more polished feel.
- Improving error handling across the application.
- Creating essential pre-release documentation like `LICENSES.md`.
- Finally, packaging your application into distributable installers for Windows and macOS, and preparing a simple way for users to download it. This phase gets you ready for your v1.0 release\!

---

Phase 3 introduces external dependencies and asynchronous logic, which can be challenging. Be patient, test incrementally, and use your AI assistants to understand how these pieces fit together. The reward is a much more powerful application\!
